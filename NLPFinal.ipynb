{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nNm1XNF9gg39"
   },
   "source": [
    "# NLP Final Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 571,
     "status": "ok",
     "timestamp": 1627784119881,
     "user": {
      "displayName": "Wei Wu",
      "photoUrl": "",
      "userId": "13527855658126139498"
     },
     "user_tz": -480
    },
    "id": "WTJO2jbZplxH"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import jieba\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 588,
     "status": "ok",
     "timestamp": 1627784120972,
     "user": {
      "displayName": "Wei Wu",
      "photoUrl": "",
      "userId": "13527855658126139498"
     },
     "user_tz": -480
    },
    "id": "oAANBFo39GpK"
   },
   "outputs": [],
   "source": [
    "from keras_preprocessing.text import Tokenizer, text_to_word_sequence\n",
    "from keras_preprocessing.sequence import pad_sequences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1627784121468,
     "user": {
      "displayName": "Wei Wu",
      "photoUrl": "",
      "userId": "13527855658126139498"
     },
     "user_tz": -480
    },
    "id": "zZkKDkxIFgGi"
   },
   "outputs": [],
   "source": [
    "jieba.setLogLevel(\"WARN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 51695,
     "status": "ok",
     "timestamp": 1627784173737,
     "user": {
      "displayName": "Wei Wu",
      "photoUrl": "",
      "userId": "13527855658126139498"
     },
     "user_tz": -480
    },
    "id": "pH282aATH8b4",
    "outputId": "b18abf38-4f20-44ba-c1fb-b820448385dc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MTF3Utyfg2XO"
   },
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "etvyHxbChEEv"
   },
   "source": [
    "This section contains the process of data preparation for the project. The class object data transform holds all the process functions for the texts. Members of the classs is defined below:\n",
    "\n",
    "\n",
    "1.   datapath :path to the files\n",
    "2.   data : the data read from the files\n",
    "3.   text cut: text after cutting\n",
    "4.   tokenizer: tokenizer object for text2sequence\n",
    "5.   label set : dictionary of labels\n",
    "6.   extraction : data extracted from the original files\n",
    "7.   fact pad sequence: fact sequence pad to the same length\n",
    "\n",
    "A list of the functions is presented as the following:\n",
    "\n",
    "1.   read data, read the data from raw files\n",
    "2.   extract data, extract the information from data and stored in extractions\n",
    "3. cut texts, perform a text cut for the facts using jieba\n",
    "4. text2sequence, convert the texts to sequence\n",
    "5. create label set,create the label set from raw files\n",
    "6. create label, create one hot vectors for the labels\n",
    "7. create label sets, create the one hot vectors label sets\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WwQwBLj29hW-"
   },
   "outputs": [],
   "source": [
    "class data_transform():\n",
    "\n",
    "  def __init__(self):\n",
    "    self.datapath = None\n",
    "    self.data = None\n",
    "    self.text_cut = None\n",
    "    self.tokenizer = None\n",
    "    self.label_set = {}\n",
    "    self.extraction = {}\n",
    "    self.fact_pad_seq = None\n",
    "  def read_data(self, path = None):\n",
    "    '''\n",
    "    Function: read_data\n",
    "    Parameters: path\n",
    "    Return: None\n",
    "    '''\n",
    "    self.datapath = path\n",
    "    f = open(path, 'r', encoding = 'utf-8')\n",
    "    raw = f.readlines()\n",
    "    data = []\n",
    "\n",
    "    for d in raw:\n",
    "      data.append(json.loads(d))\n",
    "    '''\n",
    "    for num,data_one in enumerate(raw):\n",
    "      try:\n",
    "        data.append(json.loads(data_one))\n",
    "      except Exception as e:\n",
    "        print('num : %d', '/n',\n",
    "              'error: %s', '/n',\n",
    "              'data: %s' %(num, e, data_one))\n",
    "    '''\n",
    "    self.data = data\n",
    "  def extract_data(self, name = \"fact\"):\n",
    "    '''\n",
    "    Function: extract_data\n",
    "    Parameters: name, the target information name\n",
    "    Return:None\n",
    "    '''\n",
    "    data = self.data\n",
    "    if name == \"fact\":\n",
    "      extraction = list(map(lambda x : x[\"fact\"],data))\n",
    "    elif name in [\"accusation\", \"relevent_articles\"]:\n",
    "      extraction = list(map(lambda x : x[\"meta\"][name], data))\n",
    "    elif name == \"imprisonment\":\n",
    "      extraction = []\n",
    "\n",
    "      for i in data:\n",
    "        if i[\"meta\"][\"term_of_imprisonment\"][\"death_penalty\"]:\n",
    "          extraction.append([500])\n",
    "        elif i[\"meta\"][\"term_of_imprisonment\"][\"life_imprisonment\"]:\n",
    "          extraction.append([400])\n",
    "        else:\n",
    "          extraction.append(i[\"meta\"][\"term_of_imprisonment\"][\"imprisonment\"])\n",
    "    self.extraction.update({name : extraction})\n",
    "\n",
    "  def cut_texts(self,texts = None, need_cut = True, word_len =1):\n",
    "    '''\n",
    "    Function: cut_texts \n",
    "    Parameters: texts, texts for cutting\n",
    "    need_cut, option for cut or not\n",
    "    word_len, target word length after cutting\n",
    "    Return: text cuts\n",
    "\n",
    "    '''\n",
    "      if need_cut:\n",
    "        if word_len > 1:\n",
    "          texts_cut = [[word for word in jieba.lcut(one_text) if len(word) >= word_len] for one_text in texts]\n",
    "        else:\n",
    "          texts_cut = [jieba.lcut(one_text) for one_text in texts]\n",
    "      else:\n",
    "        if word_len > 1:\n",
    "          texts_cut = [[word for word in one_text if len(word) >= word_len] for one_text in texts]\n",
    "        else:\n",
    "          texts_cut = texts\n",
    "      return texts_cut\n",
    "  def text2seq(self, texts_cut = None, tokenizer_fact = None, num_words = 2000, maxlen = 30):\n",
    "    '''\n",
    "    Function text2seq\n",
    "    Parameters: texts cut\n",
    "    tokenizer_fact, tokenizer object\n",
    "    num_words, number of words in tokenizer\n",
    "    max len, max length for thr pad sequence\n",
    "    Return: None\n",
    "    '''\n",
    "      texts_cut_len = len(texts_cut)\n",
    "      if tokenizer_fact is None:\n",
    "        tokenizer_fact = Tokenizer(num_words = num_words)\n",
    "        if texts_cut_len > 10000:\n",
    "          print(\"Too much text\")\n",
    "        n = 0\n",
    "        while n < texts_cut_len:\n",
    "          tokenizer_fact.fit_on_texts(texts = texts_cut[n:n + 10000])\n",
    "          n+= 10000\n",
    "          if n < texts_cut_len:\n",
    "            print(\"tokenizer finish fit %d samples\" % n)\n",
    "          else:\n",
    "            print(\"tokenizer finish fit %d samples\" % texts_cut_len)\n",
    "        self.tokenizer_fact = tokenizer_fact\n",
    "        \n",
    "        \n",
    "      fact_seq = tokenizer_fact.texts_to_sequences(texts = texts_cut)\n",
    "      print(\"finish texts to sequence\")\n",
    "\n",
    "      del texts_cut\n",
    "      n = 0\n",
    "\n",
    "      fact_pad_seq =[]\n",
    "\n",
    "      while n < texts_cut_len:\n",
    "        fact_pad_seq += list(pad_sequences(fact_seq[n: n + 10000],maxlen = maxlen, padding = \"post\", value =0, dtype = \"int\"))\n",
    "        n += 10000\n",
    "\n",
    "        if n < texts_cut_len:\n",
    "          print(\"finish pad sequences %d samples\" % n)\n",
    "        else:\n",
    "          print(\"finsh pad sequences %d samples\" % texts_cut_len)\n",
    "      self.fact_pad_seq = fact_pad_seq\n",
    "\n",
    "  def create_label_set(self, name):\n",
    "    '''\n",
    "    Function: create label set\n",
    "    Parameter: name, information name\n",
    "    Return None\n",
    "    '''\n",
    "      if name == \"accusation\":\n",
    "        name_f = \"accu\"\n",
    "        with open(\"/content/drive/My Drive/Colab Notebooks/%s.txt\" % name_f, encoding = \"utf-8\") as f:\n",
    "          label_set = f.readlines()\n",
    "      elif name == \"relevant_articles\":\n",
    "        name_f = \"law\"\n",
    "        with open(\"/content/drive/My Drive/Colab Notebooks/%s.txt\" % name_f, encoding = \"utf-8\") as f:\n",
    "          label_set = f.read_lines()\n",
    "      else:\n",
    "        label_set = [400,500] + list(range(1,25*12 +1))\n",
    "      label_set = [i[:-1] for i in label_set]\n",
    "      self.label_set.update({name : np.array(label_set)})\n",
    "  def create_label(self,label,label_set):\n",
    "    '''\n",
    "    Function: create label\n",
    "    Parameters: label,\n",
    "    label set\n",
    "    Return: zero one hot vector\n",
    "    '''\n",
    "      label_str = []\n",
    "      for  i in label:\n",
    "        label_str.append(str(i))\n",
    "      label_zero = np.zeros(len(label_set))\n",
    "      label_zero[np.in1d(label_set,label_str)] =1\n",
    "      return label_zero\n",
    "  def create_labels(self, label_set = None, labels = None, name= \"accusation\"):\n",
    "    '''\n",
    "    Function: create labels\n",
    "    Parameters:\n",
    "    label set, \n",
    "    labels, \n",
    "    name, name of the information\n",
    "    Return: label sets in one hot vectors\n",
    "    '''\n",
    "      if label_set is None:\n",
    "        label_set = self.label_set[name]\n",
    "      if labels is None:\n",
    "        labels = self.extraction[name]\n",
    "      labels_one_hot = list(map(lambda x: self.create_label(label =x, label_set = label_set), labels))\n",
    "      return labels_one_hot\n",
    "        \n",
    "      \n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qoMsL20GnIba"
   },
   "source": [
    "Below is the process of prepare the facts and the label data from the raw files using the functions in data_transform. The fact data is save as fact_accusation.npy ans the labels data is saved as label_accusation.npy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1le8nBqAC5kh"
   },
   "outputs": [],
   "source": [
    "data_trans = data_transform()\n",
    "data_trans.read_data(path = \"/content/drive/My Drive/Colab Notebooks/data_train.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h61AeWriO6_h"
   },
   "outputs": [],
   "source": [
    "data_trans.extract_data(name = \"accusation\")\n",
    "accusations = data_trans.extraction[\"accusation\"]\n",
    "data_trans.create_label_set(name=\"accusation\")\n",
    "accu_labels = data_trans.create_labels(name = \"accusation\")\n",
    "np.save(\"/content/drive/My Drive/Colab Notebooks/label_accusation.npy\",accu_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "T7fQE9HsJv-1",
    "outputId": "9e5c0119-e648-4fce-85e8-ce1ccb76bd63"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Too much text\n",
      "tokenizer finish fit 10000 samples\n",
      "tokenizer finish fit 20000 samples\n",
      "tokenizer finish fit 30000 samples\n",
      "tokenizer finish fit 40000 samples\n",
      "tokenizer finish fit 50000 samples\n",
      "tokenizer finish fit 60000 samples\n",
      "tokenizer finish fit 70000 samples\n",
      "tokenizer finish fit 80000 samples\n",
      "tokenizer finish fit 90000 samples\n",
      "tokenizer finish fit 100000 samples\n",
      "tokenizer finish fit 110000 samples\n",
      "tokenizer finish fit 120000 samples\n",
      "tokenizer finish fit 130000 samples\n",
      "tokenizer finish fit 140000 samples\n",
      "tokenizer finish fit 150000 samples\n",
      "tokenizer finish fit 154592 samples\n",
      "finish texts to sequence\n",
      "finish pad sequences 10000 samples\n",
      "finish pad sequences 20000 samples\n",
      "finish pad sequences 30000 samples\n",
      "finish pad sequences 40000 samples\n",
      "finish pad sequences 50000 samples\n",
      "finish pad sequences 60000 samples\n",
      "finish pad sequences 70000 samples\n",
      "finish pad sequences 80000 samples\n",
      "finish pad sequences 90000 samples\n",
      "finish pad sequences 100000 samples\n",
      "finish pad sequences 110000 samples\n",
      "finish pad sequences 120000 samples\n",
      "finish pad sequences 130000 samples\n",
      "finish pad sequences 140000 samples\n",
      "finish pad sequences 150000 samples\n",
      "finsh pad sequences 154592 samples\n"
     ]
    }
   ],
   "source": [
    "data_trans = data_transform()\n",
    "data_trans.read_data(path = \"/content/drive/My Drive/Colab Notebooks/data_train.json\")\n",
    "data_trans.extract_data(name = \"fact\")\n",
    "facts = data_trans.extraction[\"fact\"]\n",
    "texts_cut = data_trans.cut_texts(texts = facts, word_len =1, need_cut = True)\n",
    "data_trans.text2seq(texts_cut= texts_cut, tokenizer_fact = None, num_words = 40000, maxlen = 400)\n",
    "fact_seq = data_trans.fact_pad_seq\n",
    "np.save(\"/content/drive/My Drive/Colab Notebooks/fact_accu.npy\",fact_seq)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9bQtXhWLnri4"
   },
   "source": [
    "# TextCNN Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eylGUz1MocsQ"
   },
   "source": [
    "Below is the processing building the Text CNN model. The Text CNN model is a convolutional neural network for dealing with the texts. The information about the model is presented in the output.\n",
    "The TextCNN model function contains two convolutiona\n",
    "layers and a max pooling layer, the reason why they are put in a separate function is that there are three parts in the whole network with three different kernal sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 3580,
     "status": "ok",
     "timestamp": 1627784194723,
     "user": {
      "displayName": "Wei Wu",
      "photoUrl": "",
      "userId": "13527855658126139498"
     },
     "user_tz": -480
    },
    "id": "Rc6-yCFBdzCt"
   },
   "outputs": [],
   "source": [
    "import time \n",
    "import pandas as pd\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Input, Dense, Embedding, Dropout,BatchNormalization,Concatenate\n",
    "from keras.layers import Conv1D,GlobalMaxPooling1D,MaxPooling1D, Bidirectional,GRU,Flatten,Activation\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1627784194724,
     "user": {
      "displayName": "Wei Wu",
      "photoUrl": "",
      "userId": "13527855658126139498"
     },
     "user_tz": -480
    },
    "id": "UVdH-J1sNtuq"
   },
   "outputs": [],
   "source": [
    "num_words = 40000\n",
    "maxlen = 400\n",
    "filters = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 12711,
     "status": "ok",
     "timestamp": 1627784207430,
     "user": {
      "displayName": "Wei Wu",
      "photoUrl": "",
      "userId": "13527855658126139498"
     },
     "user_tz": -480
    },
    "id": "xdV--0cCN24O"
   },
   "outputs": [],
   "source": [
    "facts = np.load(\"/content/drive/My Drive/Colab Notebooks/fact_accu.npy\")\n",
    "labels = np.load(\"/content/drive/My Drive/Colab Notebooks/label_accusation.npy\")\n",
    "fact_train,fact_test = train_test_split(facts,test_size = 0.5, random_state= 1)\n",
    "labels_train,labels_test = train_test_split(labels, test_size = 0.5, random_state =1)\n",
    "\n",
    "del facts\n",
    "del labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VMDoDh-1Pgzd"
   },
   "outputs": [],
   "source": [
    "def TextCNN_model(data_input = None, kernel_size =1, filters = 256):\n",
    "  '''\n",
    "  Function: TextCNN Model\n",
    "  Parameters: data_input, input embedding layer of the data\n",
    "  kernal size,\n",
    "  filters\n",
    "  Return: the Text CNN model\n",
    "  '''\n",
    "  embed = data_input\n",
    "\n",
    "  cnn1 = Conv1D(filters,kernel_size = [kernel_size], strides =1,padding=\"same\")(embed)\n",
    "  cnn1 = BatchNormalization()(cnn1)\n",
    "  cnn1 = Activation(activation =\"relu\")(cnn1)\n",
    "  \n",
    "  cnn1 = Conv1D(filters, kernel_size = [kernel_size], strides =1, padding=\"same\")(cnn1)\n",
    "  cnn1 = BatchNormalization()(cnn1)\n",
    "  cnn1 = Activation(activation=\"relu\")(cnn1)\n",
    "\n",
    "  cnn1 = GlobalMaxPooling1D()(cnn1)\n",
    "  return cnn1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 23083123,
     "status": "ok",
     "timestamp": 1626955331859,
     "user": {
      "displayName": "Wei Wu",
      "photoUrl": "",
      "userId": "13527855658126139498"
     },
     "user_tz": -480
    },
    "id": "UXBT4WeNQclS",
    "outputId": "55584f25-0d7b-4cc8-c200-14e2aab569f0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 400)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Embedding (Embedding)           (None, 400, 512)     20480512    input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d (Conv1D)                 (None, 400, 256)     131328      Embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 400, 256)     262400      Embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, 400, 256)     393472      Embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 400, 256)     1024        conv1d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 400, 256)     1024        conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 400, 256)     1024        conv1d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 400, 256)     0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 400, 256)     0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 400, 256)     0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 400, 256)     65792       activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 400, 256)     131328      activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)               (None, 400, 256)     196864      activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 400, 256)     1024        conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 400, 256)     1024        conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 400, 256)     1024        conv1d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 400, 256)     0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 400, 256)     0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 400, 256)     0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d (GlobalMax (None, 256)          0           activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_1 (GlobalM (None, 256)          0           activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_2 (GlobalM (None, 256)          0           activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 768)          0           global_max_pooling1d[0][0]       \n",
      "                                                                 global_max_pooling1d_1[0][0]     \n",
      "                                                                 global_max_pooling1d_2[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 768)          3072        concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 500)          384500      batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 202)          101202      dense[0][0]                      \n",
      "==================================================================================================\n",
      "Total params: 22,156,614\n",
      "Trainable params: 22,152,006\n",
      "Non-trainable params: 4,608\n",
      "__________________________________________________________________________________________________\n",
      "Training Starts\n",
      "Epoch 1/5\n",
      "151/151 [==============================] - 4720s 31s/step - loss: 0.0568 - accuracy: 0.2499\n",
      "Epoch 2/5\n",
      "151/151 [==============================] - 4685s 31s/step - loss: 0.0060 - accuracy: 0.5541\n",
      "Epoch 3/5\n",
      "151/151 [==============================] - 4545s 30s/step - loss: 0.0039 - accuracy: 0.6081\n",
      "Epoch 4/5\n",
      "151/151 [==============================] - 4604s 30s/step - loss: 0.0026 - accuracy: 0.6336\n",
      "Epoch 5/5\n",
      "151/151 [==============================] - 4515s 30s/step - loss: 0.0019 - accuracy: 0.6542\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7ff352c27a10>"
      ]
     },
     "execution_count": 9,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Building the entire neural network\n",
    "\n",
    "input_layer = Input(shape = fact_train.shape[1])\n",
    "embedder = Embedding(input_dim = num_words +1,\n",
    "                     input_length = maxlen,\n",
    "                     output_dim = 512,\n",
    "                     mask_zero  = 0,\n",
    "                     name = \"Embedding\")(input_layer)\n",
    "cnn1 = TextCNN_model(embedder,1,filters)\n",
    "cnn2 = TextCNN_model(embedder,2,filters)\n",
    "cnn3 = TextCNN_model(embedder,3,filters)\n",
    "\n",
    "cnn = Concatenate(axis=1)([cnn1,cnn2,cnn3])\n",
    "cnn = BatchNormalization()(cnn)\n",
    "cnn = Dense(500,activation=\"relu\")(cnn)\n",
    "cnn = Dense(202,activation = \"sigmoid\")(cnn)\n",
    "\n",
    "opt = Adam(0.01)\n",
    "model = Model(inputs= input_layer, outputs = cnn)\n",
    "model.compile(loss =\"binary_crossentropy\", optimizer= opt, metrics = [\"accuracy\"])\n",
    "model.summary()\n",
    "\n",
    "print(\"Training Starts\")\n",
    "model.fit(x = fact_train, y = labels_train, batch_size = 512, epochs =5, verbose =1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 171
    },
    "executionInfo": {
     "elapsed": 8488,
     "status": "error",
     "timestamp": 1627786905603,
     "user": {
      "displayName": "Wei Wu",
      "photoUrl": "",
      "userId": "13527855658126139498"
     },
     "user_tz": -480
    },
    "id": "gtRtuMjqDMbV",
    "outputId": "febda6f6-d606-4015-ebb4-6495986b26df"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-5a9177c7895f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/drive/My Drive/Colab Notebooks/model.h5\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "model.save(\"/content/drive/My Drive/Colab Notebooks/model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UHjBVvJeqQdj"
   },
   "source": [
    "# Evaluations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xUbiggPnqti9"
   },
   "source": [
    "This section contains the evaluation methods of the model predicting the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m4d26_bvQc3i"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.scorer import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pVxwRlEYBxiJ"
   },
   "outputs": [],
   "source": [
    "def f1_avg(y_pred, y_true):\n",
    "  '''\n",
    "  Function: Computes the average of f1 score of micr and macro\n",
    "  Parameters: y_pred, predicted y \n",
    "  y_true, the true label\n",
    "  Return: averaged f1 score\n",
    "  '''\n",
    "  f1_m = f1_score(y_pred= y_pred, y_true= y_true, pos_label=1,average=\"micro\",zero_division= 0 )\n",
    "  f1_m1 = f1_score(y_pred = y_pred, y_true= y_true, pos_label =1,average =\"macro\",zero_division= 0)\n",
    "  return (f1_m + f1_m1)/2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mJWcA6ywiUc-"
   },
   "outputs": [],
   "source": [
    "def predict2(predictions):\n",
    "  '''\n",
    "  Function predict2,get the predictions above 0.5\n",
    "  Parameter: predictions\n",
    "  Return: list of one hot vectors of predictions above 0.5\n",
    "  '''\n",
    "  one_hots = []\n",
    "  for prediction in predictions:\n",
    "    one_hot = np.where(prediction > 0.5 ,1.0, 0.0)\n",
    "    one_hots.append(one_hot)\n",
    "  return np.array(one_hots)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HIVpIjWXiU9B"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wLhE6YH5r9N2"
   },
   "source": [
    "Below are the evaluation results of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iIBm_dLJ0NeM"
   },
   "outputs": [],
   "source": [
    "best_model = load_model(\"/content/drive/My Drive/Colab Notebooks/model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fgLlrKha_7Mq"
   },
   "outputs": [],
   "source": [
    "y_pred = best_model.predict(fact_test[:])\n",
    "print(y_pred)\n",
    "y1 = predict2(y_pred)\n",
    "print(y1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VFKVzFQGMoAw"
   },
   "outputs": [],
   "source": [
    "print(labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fXwpuX6hGPPw"
   },
   "outputs": [],
   "source": [
    "s1 = [(labels_test[i]== y1[i]).min() for i in range(len(y1))]\n",
    "print(sum(s1)/ len(s1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eT78fqsYQ_pl"
   },
   "outputs": [],
   "source": [
    "s2 =f1_avg(y_pred = y1, y_true = labels_test)\n",
    "print(s2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-7bF6sxHqZB9"
   },
   "outputs": [],
   "source": [
    "s3 = distance_score(y_true= labels_test, y_pred= y1)\n",
    "print(s3)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNGB/ChO30d+eHBT50LKQQ7",
   "collapsed_sections": [],
   "name": "NLPFinal.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
